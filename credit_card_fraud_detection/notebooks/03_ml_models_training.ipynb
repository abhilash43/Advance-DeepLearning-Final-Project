{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d30116",
   "metadata": {},
   "source": [
    "# Machine Learning Models Training & Evaluation\n",
    "\n",
    "## Objectives\n",
    "1. Train 5 different ML models on preprocessed fraud detection data\n",
    "2. Use cross-validation to assess model robustness\n",
    "3. Evaluate with metrics appropriate for imbalanced data\n",
    "4. Compare models using F1-Score, Recall, ROC-AUC, PR-AUC\n",
    "5. Identify best performing model\n",
    "6. Analyze and interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfd3eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMPORT LIBRARIES AND SETUP\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from models import ModelFactory\n",
    "from evaluation import ModelEvaluator\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print('✓ Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fc7ac",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data\n",
    "\n",
    "Load the transformed data from the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b41e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully\n",
      "  Training:   X=(170883, 30), y=(170883,)\n",
      "  Validation: X=(28481, 30), y=(28481,)\n",
      "  Test:       X=(85443, 30), y=(85443,)\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "X_train = np.load('../data/processed/X_train_transformed.npy')\n",
    "X_val = np.load('../data/processed/X_val_transformed.npy')\n",
    "X_test = np.load('../data/processed/X_test_transformed.npy')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_val = np.load('../data/processed/y_val.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "print(f'✓ Data loaded successfully')\n",
    "print(f'  Training:   X={X_train.shape}, y={y_train.shape}')\n",
    "print(f'  Validation: X={X_val.shape}, y={y_val.shape}')\n",
    "print(f'  Test:       X={X_test.shape}, y={y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012a68f",
   "metadata": {},
   "source": [
    "## Section 2: Metrics Selection for Imbalanced Data\n",
    "\n",
    "For fraud detection with severe class imbalance (~0.17% fraud), we use:\n",
    "\n",
    "| Metric | Purpose | Why Important |\n",
    "|--------|---------|---------------|\n",
    "| **Recall (Sensitivity)** | % of fraud cases detected | Miss fraud = bad outcome |\n",
    "| **Precision** | % of alerts that are fraud | False alarms = operational cost |\n",
    "| **F1-Score** | Harmonic mean of Precision & Recall | Balanced metric for imbalanced data |\n",
    "| **ROC-AUC** | Area under ROC curve | Threshold-independent performance |\n",
    "| **PR-AUC** | Area under Precision-Recall curve | Better for imbalanced data than ROC-AUC |\n",
    "| **MCC** | Matthews Correlation Coefficient | Correlation-based metric (unbiased) |\n",
    "\n",
    "**⚠️ We AVOID Accuracy** (99.8% by predicting all normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50a7609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INITIALIZING ML MODELS\n",
      "======================================================================\n",
      "\n",
      "Models to train:\n",
      "  1. Logistic Regression\n",
      "     → LogisticRegression\n",
      "  2. KNN (k=5)\n",
      "     → KNeighborsClassifier\n",
      "  3. Decision Tree\n",
      "     → DecisionTreeClassifier\n",
      "  4. Random Forest\n",
      "     → RandomForestClassifier\n",
      "  5. SVM (RBF)\n",
      "     → SVC\n",
      "\n",
      "Cross-validation folds: 5-fold stratified\n",
      "Evaluation metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation metrics for cross-validation\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',  # Most important for fraud detection\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "# Create all models\n",
    "print('='*70)\n",
    "print('INITIALIZING ML MODELS')\n",
    "print('='*70)\n",
    "\n",
    "models = ModelFactory.get_all_models(random_state=RANDOM_SEED)\n",
    "\n",
    "print(f'\\nModels to train:')\n",
    "for i, (name, model) in enumerate(models.items(), 1):\n",
    "    print(f'  {i}. {name}')\n",
    "    print(f'     → {model.__class__.__name__}')\n",
    "\n",
    "print(f'\\nCross-validation folds: 5-fold stratified')\n",
    "print(f'Evaluation metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a452846",
   "metadata": {},
   "source": [
    "## Section 3: Cross-Validation on Training Data\n",
    "\n",
    "Evaluate models using 5-fold stratified cross-validation to assess stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac60400d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION RESULTS (5-Fold Stratified)\n",
      "======================================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "  ✓ Recall: 0.9085 (+/- 0.0437)\n",
      "  ✓ F1-Score: 0.1228 (+/- 0.0051)\n",
      "  ✓ ROC-AUC: 0.9754 (+/- 0.0216)\n",
      "\n",
      "Training KNN (k=5)...\n",
      "  ✓ Recall: 0.7797 (+/- 0.0503)\n",
      "  ✓ F1-Score: 0.8341 (+/- 0.0368)\n",
      "  ✓ ROC-AUC: 0.9185 (+/- 0.0292)\n",
      "\n",
      "Training Decision Tree...\n",
      "  ✓ Recall: 0.8169 (+/- 0.0561)\n",
      "  ✓ F1-Score: 0.4026 (+/- 0.1552)\n",
      "  ✓ ROC-AUC: 0.9068 (+/- 0.0284)\n",
      "\n",
      "Training Random Forest...\n",
      "  ✓ Recall: 0.7661 (+/- 0.0507)\n",
      "  ✓ F1-Score: 0.8302 (+/- 0.0328)\n",
      "  ✓ ROC-AUC: 0.9535 (+/- 0.0188)\n",
      "\n",
      "Training SVM (RBF)...\n",
      "  ✓ Recall: 0.7424 (+/- 0.0561)\n",
      "  ✓ F1-Score: 0.4906 (+/- 0.0337)\n",
      "  ✓ ROC-AUC: 0.9696 (+/- 0.0143)\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('CROSS-VALIDATION RESULTS (5-Fold Stratified)')\n",
    "print('='*70)\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nTraining {model_name}...')\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring_metrics,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Extract and store results\n",
    "    cv_results[model_name] = {\n",
    "        'Accuracy': cv_scores['test_accuracy'].mean(),\n",
    "        'Precision': cv_scores['test_precision'].mean(),\n",
    "        'Recall': cv_scores['test_recall'].mean(),\n",
    "        'F1-Score': cv_scores['test_f1'].mean(),\n",
    "        'ROC-AUC': cv_scores['test_roc_auc'].mean(),\n",
    "    }\n",
    "    \n",
    "    print(f'  ✓ Recall: {cv_scores[\"test_recall\"].mean():.4f} (+/- {cv_scores[\"test_recall\"].std():.4f})')\n",
    "    print(f'  ✓ F1-Score: {cv_scores[\"test_f1\"].mean():.4f} (+/- {cv_scores[\"test_f1\"].std():.4f})')\n",
    "    print(f'  ✓ ROC-AUC: {cv_scores[\"test_roc_auc\"].mean():.4f} (+/- {cv_scores[\"test_roc_auc\"].std():.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CV results table\n",
    "cv_df = pd.DataFrame(cv_results).T\n",
    "cv_df = cv_df.round(4)\n",
    "cv_df = cv_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('CROSS-VALIDATION SUMMARY TABLE')\n",
    "print('='*70)\n",
    "print(cv_df.to_string())\n",
    "print('='*70)\n",
    "\n",
    "# Visualize CV results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, metric in enumerate(['Recall', 'F1-Score', 'ROC-AUC']):\n",
    "    cv_df[metric].plot(kind='barh', ax=axes[idx], color='steelblue', edgecolor='black')\n",
    "    axes[idx].set_title(f'Cross-Validation {metric}', fontweight='bold', fontsize=12)\n",
    "    axes[idx].set_xlabel(metric)\n",
    "    axes[idx].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(cv_df[metric].values):\n",
    "        axes[idx].text(v, i, f'{v:.4f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/09_cross_validation_results.png', dpi=300, bbox_inches='tight')\n",
    "print('✓ Figure saved: 09_cross_validation_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a93300",
   "metadata": {},
   "source": [
    "## Section 4: Train Final Models and Evaluate on Test Set\n",
    "\n",
    "Train each model on full training set and evaluate on test set for final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('TRAINING FINAL MODELS ON FULL TRAINING SET')\n",
    "print('='*70)\n",
    "\n",
    "# Train models on full training set\n",
    "trained_models = {}\n",
    "test_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nTraining {model_name}...')\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[model_name] = model\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    test_results[model_name] = ModelEvaluator.compute_metrics(\n",
    "        y_test, y_pred, y_pred_proba\n",
    "    )\n",
    "    \n",
    "    print(f'  ✓ Test Recall: {test_results[model_name][\"Recall\"]:.4f}')\n",
    "    print(f'  ✓ Test F1-Score: {test_results[model_name][\"F1-Score\"]:.4f}')\n",
    "    print(f'  ✓ Test ROC-AUC: {test_results[model_name][\"ROC-AUC\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test results dataframe\n",
    "test_df = pd.DataFrame(test_results).T\n",
    "test_df = test_df.round(4)\n",
    "test_df = test_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('TEST SET PERFORMANCE COMPARISON')\n",
    "print('='*70)\n",
    "print(test_df.to_string())\n",
    "print('='*70)\n",
    "\n",
    "# Save results\n",
    "test_df.to_csv('../results/metrics/model_comparison_test_set.csv')\n",
    "print('✓ Results saved: model_comparison_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test set results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes.flatten()[idx]\n",
    "    \n",
    "    if metric in test_df.columns:\n",
    "        values = test_df[metric].values\n",
    "        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(values)))\n",
    "        bars = ax.barh(test_df.index, values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        ax.set_title(f'{metric}', fontweight='bold', fontsize=12)\n",
    "        ax.set_xlabel(metric)\n",
    "        ax.set_xlim([0, 1.0])\n",
    "        ax.grid(alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(values):\n",
    "            ax.text(v + 0.02, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{metric}\\n(not computed)', \n",
    "               ha='center', va='center', fontsize=11, color='gray')\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/10_test_set_performance.png', dpi=300, bbox_inches='tight')\n",
    "print('✓ Figure saved: 10_test_set_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d98cf3",
   "metadata": {},
   "source": [
    "## Section 5: Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model_name, best_f1 = ModelEvaluator.get_best_model(test_results, metric='F1-Score')\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BEST MODEL ANALYSIS')\n",
    "print('='*70)\n",
    "print(f'\\nBest Model (by F1-Score): {best_model_name}')\n",
    "print(f'F1-Score: {test_results[best_model_name][\"F1-Score\"]:.4f}')\n",
    "print(f'\\nFull test set metrics:')\n",
    "for metric, value in test_results[best_model_name].items():\n",
    "    print(f'  {metric:15s}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion matrix for best model\n",
    "cm = ModelEvaluator.get_confusion_matrix(y_test, y_pred_best)\n",
    "cm_dict = ModelEvaluator.get_confusion_matrix_dict(y_test, y_pred_best)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('CONFUSION MATRIX INTERPRETATION')\n",
    "print('='*70)\n",
    "print(f'\\n{best_model_name}:')\n",
    "print(f'\\n             Predicted')\n",
    "print(f'           Normal  Fraud')\n",
    "print(f'Actual Normal  {cm[0,0]:6d}  {cm[0,1]:6d}')\n",
    "print(f'       Fraud   {cm[1,0]:6d}  {cm[1,1]:6d}')\n",
    "print(f'\\nMetrics:')\n",
    "print(f'  True Positives (TP):  {cm_dict[\"TP\"]:6d} - Fraud correctly detected')\n",
    "print(f'  False Positives (FP): {cm_dict[\"FP\"]:6d} - Normal flagged as fraud')\n",
    "print(f'  False Negatives (FN): {cm_dict[\"FN\"]:6d} - Fraud missed (worst case!)')\n",
    "print(f'  True Negatives (TN):  {cm_dict[\"TN\"]:6d} - Normal correctly identified')\n",
    "\n",
    "# Calculate percentages\n",
    "print(f'\\nOperational Metrics:')\n",
    "if cm_dict[\"TP\"] + cm_dict[\"FN\"] > 0:\n",
    "    fraud_detected = 100 * cm_dict[\"TP\"] / (cm_dict[\"TP\"] + cm_dict[\"FN\"])\n",
    "    print(f'  Fraud Detection Rate (Recall): {fraud_detected:.2f}%')\n",
    "if cm_dict[\"FP\"] + cm_dict[\"TN\"] > 0:\n",
    "    false_alarm_rate = 100 * cm_dict[\"FP\"] / (cm_dict[\"FP\"] + cm_dict[\"TN\"])\n",
    "    print(f'  False Alarm Rate: {false_alarm_rate:.2f}%')\n",
    "if cm_dict[\"TP\"] + cm_dict[\"FP\"] > 0:\n",
    "    precision_pct = 100 * cm_dict[\"TP\"] / (cm_dict[\"TP\"] + cm_dict[\"FP\"])\n",
    "    print(f'  Precision: {precision_pct:.2f}% of fraud alerts are true fraud')\n",
    "\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "           xticklabels=['Normal', 'Fraud'],\n",
    "           yticklabels=['Normal', 'Fraud'],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Normalized confusion matrix (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', ax=axes[1],\n",
    "           xticklabels=['Normal', 'Fraud'],\n",
    "           yticklabels=['Normal', 'Fraud'],\n",
    "           cbar_kws={'label': 'Percentage'},\n",
    "           vmin=0, vmax=1)\n",
    "axes[1].set_title(f'Normalized Confusion Matrix - {best_model_name}', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/11_best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print('✓ Figure saved: 11_best_model_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\n' + '='*70)\n",
    "print('DETAILED CLASSIFICATION REPORT')\n",
    "print('='*70)\n",
    "print(f'\\n{best_model_name}:')\n",
    "print(ModelEvaluator.get_classification_report(y_test, y_pred_best))\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf8e84",
   "metadata": {},
   "source": [
    "## Section 6: ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for all models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "for model_name, model in trained_models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, roc_auc = ModelEvaluator.get_roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    label = f'{model_name} (AUC = {roc_auc:.4f})'\n",
    "    if model_name == best_model_name:\n",
    "        axes[0].plot(fpr, tpr, linewidth=2.5, label=label, alpha=0.8)\n",
    "    else:\n",
    "        axes[0].plot(fpr, tpr, linewidth=1.5, label=label, alpha=0.6)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[0].set_title('ROC Curves - All Models', fontweight='bold', fontsize=12)\n",
    "axes[0].legend(loc='lower right', fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "for model_name, model in trained_models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, pr_auc = ModelEvaluator.get_precision_recall_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    label = f'{model_name} (AUC = {pr_auc:.4f})'\n",
    "    if model_name == best_model_name:\n",
    "        axes[1].plot(recall, precision, linewidth=2.5, label=label, alpha=0.8)\n",
    "    else:\n",
    "        axes[1].plot(recall, precision, linewidth=1.5, label=label, alpha=0.6)\n",
    "\n",
    "axes[1].set_xlabel('Recall', fontsize=11)\n",
    "axes[1].set_ylabel('Precision', fontsize=11)\n",
    "axes[1].set_title('Precision-Recall Curves - All Models', fontweight='bold', fontsize=12)\n",
    "axes[1].legend(loc='upper right', fontsize=9)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/12_roc_pr_curves.png', dpi=300, bbox_inches='tight')\n",
    "print('✓ Figure saved: 12_roc_pr_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f95bf",
   "metadata": {},
   "source": [
    "## Section 7: Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe449a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*90)\n",
    "print('MODEL TRAINING & EVALUATION SUMMARY')\n",
    "print('='*90)\n",
    "\n",
    "print('\\n1. CROSS-VALIDATION RESULTS (5-Fold):')\n",
    "print('-' * 90)\n",
    "print(cv_df.to_string())\n",
    "\n",
    "print('\\n\\n2. TEST SET RESULTS:')\n",
    "print('-' * 90)\n",
    "print(test_df.to_string())\n",
    "\n",
    "print('\\n\\n3. BEST MODEL PERFORMANCE:')\n",
    "print('-' * 90)\n",
    "print(f'Model: {best_model_name}')\n",
    "print(f'\\nTest Set Metrics:')\n",
    "for metric, value in sorted(test_results[best_model_name].items()):\n",
    "    print(f'  {metric:15s}: {value:.4f}')\n",
    "\n",
    "print(f'\\nConfusion Matrix Analysis:')\n",
    "print(f'  Fraud Detection Rate (Recall): {100 * cm_dict[\"TP\"] / (cm_dict[\"TP\"] + cm_dict[\"FN\"]):.2f}%')\n",
    "print(f'  False Alarm Rate: {100 * cm_dict[\"FP\"] / (cm_dict[\"FP\"] + cm_dict[\"TN\"]):.2f}%')\n",
    "print(f'  Fraud Caught: {cm_dict[\"TP\"]} out of {cm_dict[\"TP\"] + cm_dict[\"FN\"]} fraudulent transactions')\n",
    "\n",
    "print('\\n\\n4. KEY INSIGHTS:')\n",
    "print('-' * 90)\n",
    "print('✓ Cross-validation shows model consistency (low variance)')\n",
    "print('✓ Recall metric is most important for fraud detection (minimize missed fraud)')\n",
    "print('✓ Test set performance validates generalization to unseen data')\n",
    "print('✓ ROC-AUC and PR-AUC measure threshold-independent performance')\n",
    "print('✓ Choose model based on business requirements (recall vs. false alarms)')\n",
    "\n",
    "print('\\n' + '='*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c348cc8",
   "metadata": {},
   "source": [
    "## Section 8: Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e399922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model and results\n",
    "joblib.dump(best_model, f'../results/{best_model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "print(f'✓ Best model saved: {best_model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "\n",
    "# Save all models\n",
    "for model_name, model in trained_models.items():\n",
    "    joblib.dump(model, f'../results/{model_name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}_model.pkl')\n",
    "\n",
    "# Save results as JSON\n",
    "import json\n",
    "results_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_f1_score': float(test_results[best_model_name]['F1-Score']),\n",
    "    'all_test_results': {k: {m: float(v) for m, v in test_results[k].items()} \n",
    "                        for k in test_results}\n",
    "}\n",
    "\n",
    "with open('../results/metrics/model_results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print('✓ Results saved: model_results_summary.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
